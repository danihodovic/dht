{
  "status": "success",
  "data": {
    "groups": [
      {
        "name": "BlackboxExporter",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-blackbox-rules-d96e42d5-7027-422f-8398-992176e0e22b.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "BlackboxProbeFailed",
            "query": "probe_success == 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Probe failed\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox probe failed (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000256465,
            "lastEvaluation": "2025-05-28T20:39:11.301253417Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "BlackboxConfigurationReloadFailure",
            "query": "blackbox_exporter_config_last_reload_successful != 1",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Blackbox configuration reload failure\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox configuration reload failure (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000064551,
            "lastEvaluation": "2025-05-28T20:39:11.30151652Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "BlackboxSlowProbe",
            "query": "avg_over_time(probe_duration_seconds[1m]) > 2",
            "duration": 60,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Blackbox probe took more than 2s to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox slow probe (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000092816,
            "lastEvaluation": "2025-05-28T20:39:11.301584137Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "BlackboxProbeHttpFailure",
            "query": "probe_http_status_code <= 199 or probe_http_status_code >= 400",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox probe HTTP failure (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000084041,
            "lastEvaluation": "2025-05-28T20:39:11.301681777Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "BlackboxSslCertificateWillExpireSoon",
            "query": "3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "SSL certificate expires in less than 20 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000168118,
            "lastEvaluation": "2025-05-28T20:39:11.301768308Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "BlackboxSslCertificateWillExpireSoon",
            "query": "0 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 3",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "SSL certificate expires in less than 3 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000117867,
            "lastEvaluation": "2025-05-28T20:39:11.301940788Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "BlackboxSslCertificateExpired",
            "query": "round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Blackbox SSL certificate expired (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000089761,
            "lastEvaluation": "2025-05-28T20:39:11.302061387Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000940899,
        "lastEvaluation": "2025-05-28T20:39:11.301213266Z"
      },
      {
        "name": "alertmanager.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-alertmanager.rules-0a7f2da0-dbca-4e0c-a1d1-d3677795387a.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "AlertmanagerFailedReload",
            "query": "max_over_time(alertmanager_config_last_reload_successful{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[5m]) == 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload",
              "summary": "Reloading an Alertmanager configuration has failed."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000335205,
            "lastEvaluation": "2025-05-28T20:39:33.647805256Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerMembersInconsistent",
            "query": "max_over_time(alertmanager_cluster_members{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[5m]) < on (namespace, service, cluster) group_left () count by (namespace, service, cluster) (max_over_time(alertmanager_cluster_members{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[5m]))",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent",
              "summary": "A member of an Alertmanager cluster has not found all other cluster members."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000153426,
            "lastEvaluation": "2025-05-28T20:39:33.648147245Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerFailedToSendAlerts",
            "query": "(rate(alertmanager_notifications_failed_total{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[15m]) / ignoring (reason) group_left () rate(alertmanager_notifications_total{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[15m])) > 0.01",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts",
              "summary": "An Alertmanager instance failed to send notifications."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000872169,
            "lastEvaluation": "2025-05-28T20:39:33.648304085Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerClusterFailedToSendAlerts",
            "query": "min by (namespace, service, integration) (rate(alertmanager_notifications_failed_total{integration=~\".*\",job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[15m]) / ignoring (reason) group_left () rate(alertmanager_notifications_total{integration=~\".*\",job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[15m])) > 0.01",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts",
              "summary": "All Alertmanager instances in a cluster failed to send notifications to a critical integration."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001010995,
            "lastEvaluation": "2025-05-28T20:39:33.649183076Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerClusterFailedToSendAlerts",
            "query": "min by (namespace, service, integration) (rate(alertmanager_notifications_failed_total{integration!~\".*\",job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[15m]) / ignoring (reason) group_left () rate(alertmanager_notifications_total{integration!~\".*\",job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[15m])) > 0.01",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts",
              "summary": "All Alertmanager instances in a cluster failed to send notifications to a non-critical integration."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000188461,
            "lastEvaluation": "2025-05-28T20:39:33.650201753Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerConfigInconsistent",
            "query": "count by (namespace, service, cluster) (count_values by (namespace, service, cluster) (\"config_hash\", alertmanager_config_hash{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"})) != 1",
            "duration": 1200,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Alertmanager instances within the {{$labels.job}} cluster have different configurations.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent",
              "summary": "Alertmanager instances within the same cluster have different configurations."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000121711,
            "lastEvaluation": "2025-05-28T20:39:33.650393915Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerClusterDown",
            "query": "(count by (namespace, service, cluster) (avg_over_time(up{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[5m]) < 0.5) / count by (namespace, service, cluster) (up{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"})) >= 0.5",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown",
              "summary": "Half or more of the Alertmanager instances within the same cluster are down."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000178634,
            "lastEvaluation": "2025-05-28T20:39:33.650519069Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "AlertmanagerClusterCrashlooping",
            "query": "(count by (namespace, service, cluster) (changes(process_start_time_seconds{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"}[10m]) > 4) / count by (namespace, service, cluster) (up{job=\"kube-prometheus-stack-alertmanager\",namespace=\"kube-prometheus\"})) >= 0.5",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping",
              "summary": "Half or more of the Alertmanager instances within the same cluster are crashlooping."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000165467,
            "lastEvaluation": "2025-05-28T20:39:33.650700892Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.003103211,
        "lastEvaluation": "2025-05-28T20:39:33.647765922Z"
      },
      {
        "name": "config-reloaders",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-config-reloaders-af14c060-4204-4bf6-88a0-66afb9d7719d.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "ConfigReloaderSidecarErrors",
            "query": "max_over_time(reloader_last_reload_successful{namespace=~\".+\"}[5m]) == 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.\nAs a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors",
              "summary": "config-reloader sidecar has not had a successful reload for 10m"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00035151,
            "lastEvaluation": "2025-05-28T20:39:16.846131824Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000407425,
        "lastEvaluation": "2025-05-28T20:39:16.846081316Z"
      },
      {
        "name": "general.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-general.rules-c3601d40-9d1e-42b6-9392-38cb01af853a.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "TargetDown",
            "query": "100 * (count by (cluster, job, namespace, service) (up == 0) / count by (cluster, job, namespace, service) (up)) > 10",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/targetdown",
              "summary": "One or more targets are unreachable."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000556866,
            "lastEvaluation": "2025-05-28T20:39:07.415801483Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Watchdog",
            "query": "vector(1)",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "none"
            },
            "annotations": {
              "description": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/watchdog",
              "summary": "An alert that should always be firing to certify that Alertmanager is working properly."
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "Watchdog",
                  "severity": "none"
                },
                "annotations": {
                  "description": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n",
                  "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/watchdog",
                  "summary": "An alert that should always be firing to certify that Alertmanager is working properly."
                },
                "state": "firing",
                "activeAt": "2025-05-28T19:15:07.415336037Z",
                "value": "1e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000282806,
            "lastEvaluation": "2025-05-28T20:39:07.416366628Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "InfoInhibitor",
            "query": "ALERTS{severity=\"info\"} == 1 unless on (namespace) ALERTS{alertname!=\"InfoInhibitor\",alertstate=\"firing\",severity=~\"warning|critical\"} == 1",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "none"
            },
            "annotations": {
              "description": "This is an alert that is used to inhibit info alerts.\nBy themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with\nother alerts.\nThis alert fires whenever there's a severity=\"info\" alert, and stops firing when another alert with a\nseverity of 'warning' or 'critical' starts firing on the same namespace.\nThis alert should be routed to a null receiver and configured to inhibit alerts with severity=\"info\".\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor",
              "summary": "Info-level alert inhibition."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00013753,
            "lastEvaluation": "2025-05-28T20:39:07.416656309Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.001045384,
        "lastEvaluation": "2025-05-28T20:39:07.415751425Z"
      },
      {
        "name": "k8s.rules.container_cpu_usage_seconds_total",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.container-cpu-usage-seconds-tot-c527f647-074f-45b1-b643-6088934a30a6.yaml",
        "rules": [
          {
            "name": "node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate5m",
            "query": "sum by (cluster, namespace, pod, container) (rate(container_cpu_usage_seconds_total{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.002079545,
            "lastEvaluation": "2025-05-28T20:39:11.133836536Z",
            "type": "recording"
          },
          {
            "name": "node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate",
            "query": "sum by (cluster, namespace, pod, container) (irate(container_cpu_usage_seconds_total{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.00222674,
            "lastEvaluation": "2025-05-28T20:39:11.135927514Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.004347583,
        "lastEvaluation": "2025-05-28T20:39:11.133812358Z"
      },
      {
        "name": "k8s.rules.container_memory_cache",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.container-memory-cache-2c8d7d84-7770-4bcc-b4e2-cfae4d6034a6.yaml",
        "rules": [
          {
            "name": "node_namespace_pod_container:container_memory_cache",
            "query": "container_memory_cache{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.002050732,
            "lastEvaluation": "2025-05-28T20:39:12.846306274Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.002089824,
        "lastEvaluation": "2025-05-28T20:39:12.846272605Z"
      },
      {
        "name": "k8s.rules.container_memory_rss",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.container-memory-rss-df4457a3-20fd-4b43-8b74-39f5efa3c930.yaml",
        "rules": [
          {
            "name": "node_namespace_pod_container:container_memory_rss",
            "query": "container_memory_rss{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.00200052,
            "lastEvaluation": "2025-05-28T20:39:18.260949159Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.002082419,
        "lastEvaluation": "2025-05-28T20:39:18.260872203Z"
      },
      {
        "name": "k8s.rules.container_memory_swap",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.container-memory-swap-cffa18ab-4a39-4654-919d-f73012af6502.yaml",
        "rules": [
          {
            "name": "node_namespace_pod_container:container_memory_swap",
            "query": "container_memory_swap{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.002237334,
            "lastEvaluation": "2025-05-28T20:39:25.399904384Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.002301025,
        "lastEvaluation": "2025-05-28T20:39:25.39984651Z"
      },
      {
        "name": "k8s.rules.container_memory_working_set_bytes",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.container-memory-working-set-by-99001dd5-c85f-415f-bdad-e20f50694823.yaml",
        "rules": [
          {
            "name": "node_namespace_pod_container:container_memory_working_set_bytes",
            "query": "container_memory_working_set_bytes{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
            "health": "ok",
            "evaluationTime": 0.002130298,
            "lastEvaluation": "2025-05-28T20:39:19.112102498Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.002186268,
        "lastEvaluation": "2025-05-28T20:39:19.112051701Z"
      },
      {
        "name": "k8s.rules.container_resource",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.container-resource-ce3208d7-cfa4-4dab-a1dc-982340b7aee2.yaml",
        "rules": [
          {
            "name": "cluster:namespace:pod_memory:active:kube_pod_container_resource_requests",
            "query": "kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
            "health": "ok",
            "evaluationTime": 0.006743341,
            "lastEvaluation": "2025-05-28T20:39:14.048762172Z",
            "type": "recording"
          },
          {
            "name": "namespace_memory:kube_pod_container_resource_requests:sum",
            "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
            "health": "ok",
            "evaluationTime": 0.006320116,
            "lastEvaluation": "2025-05-28T20:39:14.05552577Z",
            "type": "recording"
          },
          {
            "name": "cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests",
            "query": "kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
            "health": "ok",
            "evaluationTime": 0.006331383,
            "lastEvaluation": "2025-05-28T20:39:14.061862055Z",
            "type": "recording"
          },
          {
            "name": "namespace_cpu:kube_pod_container_resource_requests:sum",
            "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
            "health": "ok",
            "evaluationTime": 0.006201394,
            "lastEvaluation": "2025-05-28T20:39:14.068210801Z",
            "type": "recording"
          },
          {
            "name": "cluster:namespace:pod_memory:active:kube_pod_container_resource_limits",
            "query": "kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
            "health": "ok",
            "evaluationTime": 0.004878259,
            "lastEvaluation": "2025-05-28T20:39:14.074426692Z",
            "type": "recording"
          },
          {
            "name": "namespace_memory:kube_pod_container_resource_limits:sum",
            "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
            "health": "ok",
            "evaluationTime": 0.004888276,
            "lastEvaluation": "2025-05-28T20:39:14.079321485Z",
            "type": "recording"
          },
          {
            "name": "cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits",
            "query": "kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
            "health": "ok",
            "evaluationTime": 0.003546412,
            "lastEvaluation": "2025-05-28T20:39:14.084224912Z",
            "type": "recording"
          },
          {
            "name": "namespace_cpu:kube_pod_container_resource_limits:sum",
            "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
            "health": "ok",
            "evaluationTime": 0.00348845,
            "lastEvaluation": "2025-05-28T20:39:14.087787784Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.042551967,
        "lastEvaluation": "2025-05-28T20:39:14.048729127Z"
      },
      {
        "name": "k8s.rules.pod_owner",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-k8s.rules.pod-owner-bef86322-1b95-4c1d-831f-bda1a413d608.yaml",
        "rules": [
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicaSet\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on (replicaset, namespace) group_left (owner_name) topk by (replicaset, namespace) (1, max by (replicaset, namespace, owner_name) (kube_replicaset_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "deployment"
            },
            "health": "ok",
            "evaluationTime": 0.003580304,
            "lastEvaluation": "2025-05-28T20:39:08.673280193Z",
            "type": "recording"
          },
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"DaemonSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "daemonset"
            },
            "health": "ok",
            "evaluationTime": 0.002437129,
            "lastEvaluation": "2025-05-28T20:39:08.676899955Z",
            "type": "recording"
          },
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"StatefulSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "statefulset"
            },
            "health": "ok",
            "evaluationTime": 0.000770325,
            "lastEvaluation": "2025-05-28T20:39:08.679356938Z",
            "type": "recording"
          },
          {
            "name": "namespace_workload_pod:kube_pod_owner:relabel",
            "query": "max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"Job\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
            "labels": {
              "workload_type": "job"
            },
            "health": "ok",
            "evaluationTime": 0.000191021,
            "lastEvaluation": "2025-05-28T20:39:08.680144631Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.007105618,
        "lastEvaluation": "2025-05-28T20:39:08.673234215Z"
      },
      {
        "name": "kube-prometheus-general.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kube-prometheus-general.rules-b64dc000-c3b7-423c-bfca-a60558932fdd.yaml",
        "rules": [
          {
            "name": "count:up1",
            "query": "count without (instance, pod, node) (up == 1)",
            "health": "ok",
            "evaluationTime": 0.000438365,
            "lastEvaluation": "2025-05-28T20:39:18.131516264Z",
            "type": "recording"
          },
          {
            "name": "count:up0",
            "query": "count without (instance, pod, node) (up == 0)",
            "health": "ok",
            "evaluationTime": 0.000160966,
            "lastEvaluation": "2025-05-28T20:39:18.131963133Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000646417,
        "lastEvaluation": "2025-05-28T20:39:18.131480637Z"
      },
      {
        "name": "kube-prometheus-node-recording.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kube-prometheus-node-recording.rules-2484dff7-60cd-485e-8332-d246ccc48542.yaml",
        "rules": [
          {
            "name": "instance:node_cpu:rate:sum",
            "query": "sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[3m]))",
            "health": "ok",
            "evaluationTime": 0.000279315,
            "lastEvaluation": "2025-05-28T20:39:20.523635224Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_receive_bytes:rate:sum",
            "query": "sum by (instance) (rate(node_network_receive_bytes_total[3m]))",
            "health": "ok",
            "evaluationTime": 0.000058583,
            "lastEvaluation": "2025-05-28T20:39:20.5239212Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_transmit_bytes:rate:sum",
            "query": "sum by (instance) (rate(node_network_transmit_bytes_total[3m]))",
            "health": "ok",
            "evaluationTime": 0.000048581,
            "lastEvaluation": "2025-05-28T20:39:20.523983075Z",
            "type": "recording"
          },
          {
            "name": "instance:node_cpu:ratio",
            "query": "sum without (cpu, mode) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])) / on (instance) group_left () count by (instance) (sum by (instance, cpu) (node_cpu_seconds_total))",
            "health": "ok",
            "evaluationTime": 0.00010688,
            "lastEvaluation": "2025-05-28T20:39:20.524034557Z",
            "type": "recording"
          },
          {
            "name": "cluster:node_cpu:sum_rate5m",
            "query": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.000068369,
            "lastEvaluation": "2025-05-28T20:39:20.524144381Z",
            "type": "recording"
          },
          {
            "name": "cluster:node_cpu:ratio",
            "query": "cluster:node_cpu:sum_rate5m / count(sum by (instance, cpu) (node_cpu_seconds_total))",
            "health": "ok",
            "evaluationTime": 0.00005072,
            "lastEvaluation": "2025-05-28T20:39:20.524215373Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000674946,
        "lastEvaluation": "2025-05-28T20:39:20.523594226Z"
      },
      {
        "name": "kube-state-metrics",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kube-state-metrics-8456ea02-809a-4149-b242-782628ec0a49.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeStateMetricsListErrors",
            "query": "(sum by (cluster) (rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum by (cluster) (rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors",
              "summary": "kube-state-metrics is experiencing errors in list operations."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000334066,
            "lastEvaluation": "2025-05-28T20:39:26.194488381Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStateMetricsWatchErrors",
            "query": "(sum by (cluster) (rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum by (cluster) (rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors",
              "summary": "kube-state-metrics is experiencing errors in watch operations."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000166565,
            "lastEvaluation": "2025-05-28T20:39:26.194830257Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStateMetricsShardingMismatch",
            "query": "stdvar by (cluster) (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) != 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch",
              "summary": "kube-state-metrics sharding is misconfigured."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000061393,
            "lastEvaluation": "2025-05-28T20:39:26.195001042Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStateMetricsShardsMissing",
            "query": "2 ^ max by (cluster) (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) - 1 - sum by (cluster) (2 ^ max by (cluster, shard_ordinal) (kube_state_metrics_shard_ordinal{job=\"kube-state-metrics\"})) != 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing",
              "summary": "kube-state-metrics shards are missing."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000124048,
            "lastEvaluation": "2025-05-28T20:39:26.195065191Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000734641,
        "lastEvaluation": "2025-05-28T20:39:26.194457853Z"
      },
      {
        "name": "kubelet.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubelet.rules-01866d5f-2b09-4ff2-9c46-df6a2db6c1ba.yaml",
        "rules": [
          {
            "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
            "labels": {
              "quantile": "0.99"
            },
            "health": "ok",
            "evaluationTime": 0.000356448,
            "lastEvaluation": "2025-05-28T20:39:27.439435223Z",
            "type": "recording"
          },
          {
            "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.9, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
            "labels": {
              "quantile": "0.9"
            },
            "health": "ok",
            "evaluationTime": 0.000147357,
            "lastEvaluation": "2025-05-28T20:39:27.439798648Z",
            "type": "recording"
          },
          {
            "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
            "query": "histogram_quantile(0.5, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
            "labels": {
              "quantile": "0.5"
            },
            "health": "ok",
            "evaluationTime": 0.000122226,
            "lastEvaluation": "2025-05-28T20:39:27.439949966Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000710019,
        "lastEvaluation": "2025-05-28T20:39:27.439364986Z"
      },
      {
        "name": "kubernetes-apps",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubernetes-apps-84df7000-0244-4d97-9153-fc946050a2b1.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubePodCrashLooping",
            "query": "max_over_time(kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\".*\",reason=\"CrashLoopBackOff\"}[5m]) >= 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: \"CrashLoopBackOff\") on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping",
              "summary": "Pod is crash looping."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000261271,
            "lastEvaluation": "2025-05-28T20:39:35.628312265Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDeploymentGenerationMismatch",
            "query": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\".*\"}",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch",
              "summary": "Deployment generation mismatch due to possible roll-back"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000461796,
            "lastEvaluation": "2025-05-28T20:39:35.628581403Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDeploymentReplicasMismatch",
            "query": "(kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} > kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\".*\"}) and (changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[10m]) == 0)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch",
              "summary": "Deployment has not matched the expected number of replicas."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000663439,
            "lastEvaluation": "2025-05-28T20:39:35.629049342Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDeploymentRolloutStuck",
            "query": "kube_deployment_status_condition{condition=\"Progressing\",job=\"kube-state-metrics\",namespace=~\".*\",status=\"false\"} != 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck",
              "summary": "Deployment rollout is not progressing."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000293881,
            "lastEvaluation": "2025-05-28T20:39:35.629717872Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStatefulSetReplicasMismatch",
            "query": "(kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[10m]) == 0)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch",
              "summary": "StatefulSet has not matched the expected number of replicas."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000304193,
            "lastEvaluation": "2025-05-28T20:39:35.630016077Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStatefulSetGenerationMismatch",
            "query": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\".*\"}",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch",
              "summary": "StatefulSet generation mismatch due to possible roll-back"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000144007,
            "lastEvaluation": "2025-05-28T20:39:35.630324674Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeStatefulSetUpdateNotRolledOut",
            "query": "(max by (namespace, statefulset, job, cluster) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\".*\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\".*\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"})) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout",
              "summary": "StatefulSet update has not been rolled out."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000441328,
            "lastEvaluation": "2025-05-28T20:39:35.630472258Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDaemonSetRolloutStuck",
            "query": "((kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}) or (kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != 0) or (kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}) or (kube_daemonset_status_number_available{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"})) and (changes(kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15m on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck",
              "summary": "DaemonSet rollout is stuck."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001658101,
            "lastEvaluation": "2025-05-28T20:39:35.63091923Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeContainerWaiting",
            "query": "kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\".*\",reason!=\"CrashLoopBackOff\"} > 0",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour. (reason: \"{{ $labels.reason }}\") on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting",
              "summary": "Pod container waiting longer than 1 hour"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000129623,
            "lastEvaluation": "2025-05-28T20:39:35.632590318Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDaemonSetNotScheduled",
            "query": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled",
              "summary": "DaemonSet pods are not scheduled."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000474959,
            "lastEvaluation": "2025-05-28T20:39:35.632723224Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeDaemonSetMisScheduled",
            "query": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled",
              "summary": "DaemonSet pods are misscheduled."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000240729,
            "lastEvaluation": "2025-05-28T20:39:35.633201794Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeJobNotCompleted",
            "query": "time() - max by (namespace, job_name, cluster) (kube_job_status_start_time{job=\"kube-state-metrics\",namespace=~\".*\"} and kube_job_status_active{job=\"kube-state-metrics\",namespace=~\".*\"} > 0) > 43200",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ \"43200\" | humanizeDuration }} to complete on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted",
              "summary": "Job did not complete in time"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000150006,
            "lastEvaluation": "2025-05-28T20:39:35.633445832Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeJobFailed",
            "query": "kube_job_failed{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed",
              "summary": "Job failed to complete."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00005868,
            "lastEvaluation": "2025-05-28T20:39:35.633598967Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeHpaReplicasMismatch",
            "query": "(kube_horizontalpodautoscaler_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} > kube_horizontalpodautoscaler_spec_min_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} < kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and changes(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}[15m]) == 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch",
              "summary": "HPA has not matched desired number of replicas."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000335365,
            "lastEvaluation": "2025-05-28T20:39:35.633661469Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeHpaMaxedOut",
            "query": "kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} == kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout",
              "summary": "HPA is running at max replicas"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000089047,
            "lastEvaluation": "2025-05-28T20:39:35.634000631Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePdbNotEnoughHealthyPods",
            "query": "(kube_poddisruptionbudget_status_desired_healthy{job=\"kube-state-metrics\",namespace=~\".*\"} - kube_poddisruptionbudget_status_current_healthy{job=\"kube-state-metrics\",namespace=~\".*\"}) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "PDB {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.poddisruptionbudget }} expects {{ $value }} more healthy pods. The desired number of healthy pods has not been met for at least 15m.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepdbnotenoughhealthypods",
              "summary": "PDB does not have enough healthy pods."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000108618,
            "lastEvaluation": "2025-05-28T20:39:35.63409435Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.005930562,
        "lastEvaluation": "2025-05-28T20:39:35.62827529Z"
      },
      {
        "name": "kubernetes-resources",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubernetes-resources-d208b496-f204-458d-8ab6-498aa46f06ac.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeCPUQuotaOvercommit",
            "query": "sum by (cluster) (min without (resource) (kube_resourcequota{job=\"kube-state-metrics\",resource=~\"(cpu|requests.cpu)\",type=\"hard\"})) / sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) > 1.5",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Cluster {{ $labels.cluster }}  has overcommitted CPU resource requests for Namespaces.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit",
              "summary": "Cluster has overcommitted CPU resource requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00050634,
            "lastEvaluation": "2025-05-28T20:39:32.257579275Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeMemoryQuotaOvercommit",
            "query": "sum by (cluster) (min without (resource) (kube_resourcequota{job=\"kube-state-metrics\",resource=~\"(memory|requests.memory)\",type=\"hard\"})) / sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"}) > 1.5",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Namespaces.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit",
              "summary": "Cluster has overcommitted memory resource requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000272013,
            "lastEvaluation": "2025-05-28T20:39:32.258094524Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeQuotaAlmostFull",
            "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) > 0.9 < 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull",
              "summary": "Namespace quota is going to be full."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000245744,
            "lastEvaluation": "2025-05-28T20:39:32.258370795Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeQuotaFullyUsed",
            "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) == 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused",
              "summary": "Namespace quota is fully used."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00014023,
            "lastEvaluation": "2025-05-28T20:39:32.258621174Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeQuotaExceeded",
            "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) > 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded",
              "summary": "Namespace quota has exceeded the limits."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000158325,
            "lastEvaluation": "2025-05-28T20:39:32.258764961Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CPUThrottlingHigh",
            "query": "sum without (id, metrics_path, name, image, endpoint, job, node) (increase(container_cpu_cfs_throttled_periods_total{container!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) / on (cluster, namespace, pod, container, instance) group_left () sum without (id, metrics_path, name, image, endpoint, job, node) (increase(container_cpu_cfs_periods_total{job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) > (25 / 100)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh",
              "summary": "Processes experience elevated CPU throttling."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000206772,
            "lastEvaluation": "2025-05-28T20:39:32.25892655Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.001593419,
        "lastEvaluation": "2025-05-28T20:39:32.257542842Z"
      },
      {
        "name": "kubernetes-storage",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubernetes-storage-4af05725-3e03-4b9c-9b87-107b4be066ec.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubePersistentVolumeFillingUp",
            "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.03 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
            "duration": 60,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup",
              "summary": "PersistentVolume is filling up."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000463844,
            "lastEvaluation": "2025-05-28T20:39:17.056115043Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePersistentVolumeFillingUp",
            "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.15 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}[6h], 4 * 24 * 3600) < 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup",
              "summary": "PersistentVolume is filling up."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000395118,
            "lastEvaluation": "2025-05-28T20:39:17.05658762Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePersistentVolumeInodesFillingUp",
            "query": "(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.03 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
            "duration": 60,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup",
              "summary": "PersistentVolumeInodes are filling up."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000201675,
            "lastEvaluation": "2025-05-28T20:39:17.056987653Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePersistentVolumeInodesFillingUp",
            "query": "(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.15 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 and predict_linear(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}[6h], 4 * 24 * 3600) < 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup",
              "summary": "PersistentVolumeInodes are filling up."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000302285,
            "lastEvaluation": "2025-05-28T20:39:17.057192604Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubePersistentVolumeErrors",
            "query": "kube_persistentvolume_status_phase{job=\"kube-state-metrics\",phase=~\"Failed|Pending\"} > 0",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors",
              "summary": "PersistentVolume is having issues with provisioning."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000280098,
            "lastEvaluation": "2025-05-28T20:39:17.057498959Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.001701102,
        "lastEvaluation": "2025-05-28T20:39:17.056082284Z"
      },
      {
        "name": "kubernetes-system",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubernetes-system-0f373113-643a-4b0c-b2c3-6999feb5961e.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeVersionMismatch",
            "query": "count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~\"kube-dns|coredns\"}, \"git_version\", \"$1\", \"git_version\", \"(v[0-9]*.[0-9]*).*\"))) > 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "There are {{ $value }} different semantic versions of Kubernetes components running on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch",
              "summary": "Different semantic versions of Kubernetes components running."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000359875,
            "lastEvaluation": "2025-05-28T20:39:32.468444156Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeClientErrors",
            "query": "(sum by (cluster, instance, job, namespace) (rate(rest_client_requests_total{code=~\"5..\",job=\"apiserver\"}[5m])) / sum by (cluster, instance, job, namespace) (rate(rest_client_requests_total{job=\"apiserver\"}[5m]))) > 0.01",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors",
              "summary": "Kubernetes API server client is experiencing errors."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000218194,
            "lastEvaluation": "2025-05-28T20:39:32.468810173Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000628305,
        "lastEvaluation": "2025-05-28T20:39:32.468402845Z"
      },
      {
        "name": "kubernetes-system-apiserver",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubernetes-system-apiserver-8aa7a03b-8963-4c79-9423-18fe639b369e.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeClientCertificateExpiration",
            "query": "histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 604800 and on (job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration",
              "summary": "Client certificate is about to expire."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000350897,
            "lastEvaluation": "2025-05-28T20:39:13.994446431Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeClientCertificateExpiration",
            "query": "histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 86400 and on (job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration",
              "summary": "Client certificate is about to expire."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000167852,
            "lastEvaluation": "2025-05-28T20:39:13.994803406Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeAggregatedAPIDown",
            "query": "(1 - max by (name, namespace, cluster) (avg_over_time(aggregator_unavailable_apiservice{job=\"apiserver\"}[10m]))) * 100 < 85",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown",
              "summary": "Kubernetes aggregated API is down."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00011219,
            "lastEvaluation": "2025-05-28T20:39:13.994975547Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeAPITerminatedRequests",
            "query": "sum by (cluster) (rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m])) / (sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\"}[10m])) + sum by (cluster) (rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m]))) > 0.2",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests",
              "summary": "The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000150144,
            "lastEvaluation": "2025-05-28T20:39:13.995091086Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000837109,
        "lastEvaluation": "2025-05-28T20:39:13.994407009Z"
      },
      {
        "name": "kubernetes-system-kubelet",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-kubernetes-system-kubelet-03434992-fc53-44da-b3e6-291cc2cd6c20.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "KubeNodeNotReady",
            "query": "kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0 and on (cluster, node) kube_node_spec_unschedulable{job=\"kube-state-metrics\"} == 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.node }} has been unready for more than 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready",
              "summary": "Node is not ready."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000675536,
            "lastEvaluation": "2025-05-28T20:39:08.870997618Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeNodePressure",
            "query": "kube_node_status_condition{condition=~\"(MemoryPressure|DiskPressure|PIDPressure)\",job=\"kube-state-metrics\",status=\"true\"} == 1 and on (cluster, node) kube_node_spec_unschedulable{job=\"kube-state-metrics\"} == 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "{{ $labels.node }} on cluster {{ $labels.cluster }} has active Condition {{ $labels.condition }}. This is caused by resource usage exceeding eviction thresholds.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodepressure",
              "summary": "Node has as active Condition."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00072136,
            "lastEvaluation": "2025-05-28T20:39:08.871687865Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeNodeUnreachable",
            "query": "(kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} unless ignoring (key, value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.node }} is unreachable and some workloads may be rescheduled on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable",
              "summary": "Node is unreachable."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000324988,
            "lastEvaluation": "2025-05-28T20:39:08.872417232Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeNodeReadinessFlapping",
            "query": "sum by (cluster, node) (changes(kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"}[15m])) > 2 and on (cluster, node) kube_node_spec_unschedulable{job=\"kube-state-metrics\"} == 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping",
              "summary": "Node readiness status is flapping."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000574904,
            "lastEvaluation": "2025-05-28T20:39:08.872749464Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeNodeEviction",
            "query": "sum by (cluster, eviction_signal, instance) (rate(kubelet_evictions{job=\"kubelet\",metrics_path=\"/metrics\"}[15m])) * on (cluster, instance) group_left (node) max by (cluster, instance, node) (kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"}) > 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "Node {{ $labels.node }} on {{ $labels.cluster }} is evicting Pods due to {{ $labels.eviction_signal }}.  Eviction occurs when eviction thresholds are crossed, typically caused by Pods exceeding RAM/ephemeral-storage limits.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeeviction",
              "summary": "Node is evicting pods."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000232984,
            "lastEvaluation": "2025-05-28T20:39:08.873331186Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletPlegDurationHigh",
            "query": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh",
              "summary": "Kubelet Pod Lifecycle Event Generator is taking too long to relist."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000080282,
            "lastEvaluation": "2025-05-28T20:39:08.873571569Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletPodStartUpLatencyHigh",
            "query": "histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m]))) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"} > 60",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh",
              "summary": "Kubelet Pod startup latency is too high."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000204866,
            "lastEvaluation": "2025-05-28T20:39:08.873656414Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletClientCertificateExpiration",
            "query": "kubelet_certificate_manager_client_ttl_seconds < 604800",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration",
              "summary": "Kubelet client certificate is about to expire."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000113802,
            "lastEvaluation": "2025-05-28T20:39:08.873866307Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletClientCertificateExpiration",
            "query": "kubelet_certificate_manager_client_ttl_seconds < 86400",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration",
              "summary": "Kubelet client certificate is about to expire."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000066301,
            "lastEvaluation": "2025-05-28T20:39:08.873984782Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletServerCertificateExpiration",
            "query": "kubelet_certificate_manager_server_ttl_seconds < 604800",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration",
              "summary": "Kubelet server certificate is about to expire."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000073532,
            "lastEvaluation": "2025-05-28T20:39:08.874054617Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletServerCertificateExpiration",
            "query": "kubelet_certificate_manager_server_ttl_seconds < 86400",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }} on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration",
              "summary": "Kubelet server certificate is about to expire."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000056911,
            "lastEvaluation": "2025-05-28T20:39:08.874132302Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletClientCertificateRenewalErrors",
            "query": "increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes) on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors",
              "summary": "Kubelet has failed to renew its client certificate."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000097121,
            "lastEvaluation": "2025-05-28T20:39:08.874193202Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "KubeletServerCertificateRenewalErrors",
            "query": "increase(kubelet_server_expiration_renew_errors[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes) on cluster {{ $labels.cluster }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors",
              "summary": "Kubelet has failed to renew its server certificate."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000082493,
            "lastEvaluation": "2025-05-28T20:39:08.874294609Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.003455095,
        "lastEvaluation": "2025-05-28T20:39:08.870925427Z"
      },
      {
        "name": "node-exporter",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-node-exporter-e743ea37-cdec-42c1-838f-c517c8ea06d8.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "NodeFilesystemSpaceFillingUp",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 15 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup",
              "summary": "Filesystem is predicted to run out of space within the next 24 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000432643,
            "lastEvaluation": "2025-05-28T20:39:24.453920548Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemSpaceFillingUp",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 10 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup",
              "summary": "Filesystem is predicted to run out of space within the next 4 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000220384,
            "lastEvaluation": "2025-05-28T20:39:24.454360696Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfSpace",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 1800,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace",
              "summary": "Filesystem has less than 5% space left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000109336,
            "lastEvaluation": "2025-05-28T20:39:24.454584821Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfSpace",
            "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 1800,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace",
              "summary": "Filesystem has less than 3% space left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000154048,
            "lastEvaluation": "2025-05-28T20:39:24.454697678Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemFilesFillingUp",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup",
              "summary": "Filesystem is predicted to run out of inodes within the next 24 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000545334,
            "lastEvaluation": "2025-05-28T20:39:24.454858628Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemFilesFillingUp",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup",
              "summary": "Filesystem is predicted to run out of inodes within the next 4 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000266956,
            "lastEvaluation": "2025-05-28T20:39:24.455415701Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfFiles",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles",
              "summary": "Filesystem has less than 5% inodes left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000111067,
            "lastEvaluation": "2025-05-28T20:39:24.455687392Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFilesystemAlmostOutOfFiles",
            "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles",
              "summary": "Filesystem has less than 3% inodes left."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00017367,
            "lastEvaluation": "2025-05-28T20:39:24.455801338Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeNetworkReceiveErrs",
            "query": "rate(node_network_receive_errs_total{job=\"node-exporter\"}[2m]) / rate(node_network_receive_packets_total{job=\"node-exporter\"}[2m]) > 0.01",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs",
              "summary": "Network interface is reporting many receive errors."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000103655,
            "lastEvaluation": "2025-05-28T20:39:24.455979143Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeNetworkTransmitErrs",
            "query": "rate(node_network_transmit_errs_total{job=\"node-exporter\"}[2m]) / rate(node_network_transmit_packets_total{job=\"node-exporter\"}[2m]) > 0.01",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs",
              "summary": "Network interface is reporting many transmit errors."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000081379,
            "lastEvaluation": "2025-05-28T20:39:24.456085671Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeHighNumberConntrackEntriesUsed",
            "query": "(node_nf_conntrack_entries{job=\"node-exporter\"} / node_nf_conntrack_entries_limit) > 0.75",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $labels.instance }} {{ $value | humanizePercentage }} of conntrack entries are used.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused",
              "summary": "Number of conntrack are getting close to the limit."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00005988,
            "lastEvaluation": "2025-05-28T20:39:24.456170167Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeTextFileCollectorScrapeError",
            "query": "node_textfile_scrape_error{job=\"node-exporter\"} == 1",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Node Exporter text file collector on {{ $labels.instance }} failed to scrape.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror",
              "summary": "Node Exporter text file collector failed to scrape."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000040329,
            "lastEvaluation": "2025-05-28T20:39:24.45623369Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeClockSkewDetected",
            "query": "(node_timex_offset_seconds{job=\"node-exporter\"} > 0.05 and deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) >= 0) or (node_timex_offset_seconds{job=\"node-exporter\"} < -0.05 and deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) <= 0)",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected",
              "summary": "Clock skew detected."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000161489,
            "lastEvaluation": "2025-05-28T20:39:24.456276345Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeClockNotSynchronising",
            "query": "min_over_time(node_timex_sync_status{job=\"node-exporter\"}[5m]) == 0 and node_timex_maxerror_seconds{job=\"node-exporter\"} >= 16",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising",
              "summary": "Clock not synchronising."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000075996,
            "lastEvaluation": "2025-05-28T20:39:24.456440597Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeRAIDDegraded",
            "query": "node_md_disks_required{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"} - ignoring (state) (node_md_disks{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\",state=\"active\"}) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded",
              "summary": "RAID Array is degraded."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00020757,
            "lastEvaluation": "2025-05-28T20:39:24.45651893Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeRAIDDiskFailure",
            "query": "node_md_disks{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\",state=\"failed\"} > 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure",
              "summary": "Failed device in RAID array."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000094629,
            "lastEvaluation": "2025-05-28T20:39:24.456730076Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFileDescriptorLimit",
            "query": "(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 70)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit",
              "summary": "Kernel is predicted to exhaust file descriptors limit soon."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000126057,
            "lastEvaluation": "2025-05-28T20:39:24.456829391Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeFileDescriptorLimit",
            "query": "(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 90)",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit",
              "summary": "Kernel is predicted to exhaust file descriptors limit soon."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000077829,
            "lastEvaluation": "2025-05-28T20:39:24.456960391Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeCPUHighUsage",
            "query": "sum without (mode) (avg without (cpu) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode!~\"idle|iowait\"}[2m]))) * 100 > 90",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}%.\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage",
              "summary": "High CPU usage."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000116487,
            "lastEvaluation": "2025-05-28T20:39:24.457043041Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeSystemSaturation",
            "query": "node_load1{job=\"node-exporter\"} / count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"}) > 2",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis might indicate this instance resources saturation and can cause it becoming unresponsive.\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation",
              "summary": "System saturated, load per core is very high."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000080382,
            "lastEvaluation": "2025-05-28T20:39:24.457163884Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeMemoryMajorPagesFaults",
            "query": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[5m]) > 500",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nPlease check that there is enough memory available at this instance.\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults",
              "summary": "Memory major page faults are occurring at very high rate."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000055952,
            "lastEvaluation": "2025-05-28T20:39:24.457247276Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeMemoryHighUtilization",
            "query": "100 - (node_memory_MemAvailable_bytes{job=\"node-exporter\"} / node_memory_MemTotal_bytes{job=\"node-exporter\"} * 100) > 90",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}%.\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization",
              "summary": "Host is running out of memory."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000090166,
            "lastEvaluation": "2025-05-28T20:39:24.457324863Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeDiskIOSaturation",
            "query": "rate(node_disk_io_time_weighted_seconds_total{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"}[5m]) > 10",
            "duration": 1800,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis symptom might indicate disk saturation.\n",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation",
              "summary": "Disk IO queue is high."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000125689,
            "lastEvaluation": "2025-05-28T20:39:24.457417494Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeSystemdServiceFailed",
            "query": "node_systemd_unit_state{job=\"node-exporter\",state=\"failed\"} == 1",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed",
              "summary": "Systemd service has entered failed state."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000048817,
            "lastEvaluation": "2025-05-28T20:39:24.45754636Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeSystemdServiceCrashlooping",
            "query": "increase(node_systemd_service_restart_total{job=\"node-exporter\"}[5m]) > 2",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Systemd service {{ $labels.name }} has being restarted too many times at {{ $labels.instance }} for the last 15 minutes. Please check if service is crash looping.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicecrashlooping",
              "summary": "Systemd service keeps restaring, possibly crash looping."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000053315,
            "lastEvaluation": "2025-05-28T20:39:24.457597958Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "NodeBondingDegraded",
            "query": "(node_bonding_slaves - node_bonding_active) != 0",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Bonding interface {{ $labels.master }} on {{ $labels.instance }} is in degraded state due to one or more slave failures.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded",
              "summary": "Bonding interface is degraded"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000053698,
            "lastEvaluation": "2025-05-28T20:39:24.457653907Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.003852907,
        "lastEvaluation": "2025-05-28T20:39:24.453857828Z"
      },
      {
        "name": "node-exporter.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-node-exporter.rules-806a1539-2a53-48a7-9522-83dbe07ec7b7.yaml",
        "rules": [
          {
            "name": "instance:node_num_cpu:sum",
            "query": "count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"})",
            "health": "ok",
            "evaluationTime": 0.000218083,
            "lastEvaluation": "2025-05-28T20:39:35.146220647Z",
            "type": "recording"
          },
          {
            "name": "instance:node_cpu_utilisation:rate5m",
            "query": "1 - avg without (cpu) (sum without (mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode=~\"idle|iowait|steal\"}[5m])))",
            "health": "ok",
            "evaluationTime": 0.000150812,
            "lastEvaluation": "2025-05-28T20:39:35.146446617Z",
            "type": "recording"
          },
          {
            "name": "instance:node_load1_per_cpu:ratio",
            "query": "(node_load1{job=\"node-exporter\"} / instance:node_num_cpu:sum{job=\"node-exporter\"})",
            "health": "ok",
            "evaluationTime": 0.000068292,
            "lastEvaluation": "2025-05-28T20:39:35.146601176Z",
            "type": "recording"
          },
          {
            "name": "instance:node_memory_utilisation:ratio",
            "query": "1 - ((node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"})) / node_memory_MemTotal_bytes{job=\"node-exporter\"})",
            "health": "ok",
            "evaluationTime": 0.000253237,
            "lastEvaluation": "2025-05-28T20:39:35.146672335Z",
            "type": "recording"
          },
          {
            "name": "instance:node_vmstat_pgmajfault:rate5m",
            "query": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[5m])",
            "health": "ok",
            "evaluationTime": 0.000054145,
            "lastEvaluation": "2025-05-28T20:39:35.14693051Z",
            "type": "recording"
          },
          {
            "name": "instance_device:node_disk_io_time_seconds:rate5m",
            "query": "rate(node_disk_io_time_seconds_total{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"}[5m])",
            "health": "ok",
            "evaluationTime": 0.000131078,
            "lastEvaluation": "2025-05-28T20:39:35.146987241Z",
            "type": "recording"
          },
          {
            "name": "instance_device:node_disk_io_time_weighted_seconds:rate5m",
            "query": "rate(node_disk_io_time_weighted_seconds_total{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"}[5m])",
            "health": "ok",
            "evaluationTime": 0.000092127,
            "lastEvaluation": "2025-05-28T20:39:35.147121312Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_receive_bytes_excluding_lo:rate5m",
            "query": "sum without (device) (rate(node_network_receive_bytes_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.00005657,
            "lastEvaluation": "2025-05-28T20:39:35.147218258Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_transmit_bytes_excluding_lo:rate5m",
            "query": "sum without (device) (rate(node_network_transmit_bytes_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.000050944,
            "lastEvaluation": "2025-05-28T20:39:35.147278101Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_receive_drop_excluding_lo:rate5m",
            "query": "sum without (device) (rate(node_network_receive_drop_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.000045808,
            "lastEvaluation": "2025-05-28T20:39:35.147331551Z",
            "type": "recording"
          },
          {
            "name": "instance:node_network_transmit_drop_excluding_lo:rate5m",
            "query": "sum without (device) (rate(node_network_transmit_drop_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
            "health": "ok",
            "evaluationTime": 0.000049839,
            "lastEvaluation": "2025-05-28T20:39:35.147380792Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.001256335,
        "lastEvaluation": "2025-05-28T20:39:35.146177104Z"
      },
      {
        "name": "node-network",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-node-network-31cc4628-39b9-47ac-9aff-3af0f7b6e650.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "NodeNetworkInterfaceFlapping",
            "query": "changes(node_network_up{device!~\"veth.+\",job=\"node-exporter\"}[2m]) > 2",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Network interface \"{{ $labels.device }}\" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping",
              "summary": "Network interface is often changing its status"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000259517,
            "lastEvaluation": "2025-05-28T20:39:22.748362884Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000305241,
        "lastEvaluation": "2025-05-28T20:39:22.748321029Z"
      },
      {
        "name": "node.rules",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-node.rules-05aec165-b554-45a7-85ad-70185a31d8fe.yaml",
        "rules": [
          {
            "name": "node_namespace_pod:kube_pod_info:",
            "query": "topk by (cluster, namespace, pod) (1, max by (cluster, node, namespace, pod) (label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")))",
            "health": "ok",
            "evaluationTime": 0.002901029,
            "lastEvaluation": "2025-05-28T20:39:24.354595875Z",
            "type": "recording"
          },
          {
            "name": "node:node_num_cpu:sum",
            "query": "count by (cluster, node) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:))",
            "health": "ok",
            "evaluationTime": 0.00121057,
            "lastEvaluation": "2025-05-28T20:39:24.357523853Z",
            "type": "recording"
          },
          {
            "name": ":node_memory_MemAvailable_bytes:sum",
            "query": "sum by (cluster) (node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))",
            "health": "ok",
            "evaluationTime": 0.000190226,
            "lastEvaluation": "2025-05-28T20:39:24.358743413Z",
            "type": "recording"
          },
          {
            "name": "node:node_cpu_utilization:ratio_rate5m",
            "query": "avg by (cluster, node) (sum without (mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])))",
            "health": "ok",
            "evaluationTime": 0.000124377,
            "lastEvaluation": "2025-05-28T20:39:24.3589394Z",
            "type": "recording"
          },
          {
            "name": "cluster:node_cpu:ratio_rate5m",
            "query": "avg by (cluster) (node:node_cpu_utilization:ratio_rate5m)",
            "health": "ok",
            "evaluationTime": 0.000077015,
            "lastEvaluation": "2025-05-28T20:39:24.359069441Z",
            "type": "recording"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.004589479,
        "lastEvaluation": "2025-05-28T20:39:24.354560129Z"
      },
      {
        "name": "prometheus",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-prometheus-89f23a06-fc18-478f-b2c6-2b1f2f0e7fb9.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "PrometheusBadConfig",
            "query": "max_over_time(prometheus_config_last_reload_successful{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) == 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig",
              "summary": "Failed Prometheus configuration reload."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000258294,
            "lastEvaluation": "2025-05-28T20:39:25.960933289Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusSDRefreshFailure",
            "query": "increase(prometheus_sd_refresh_failures_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[10m]) > 0",
            "duration": 1200,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to refresh SD with mechanism {{$labels.mechanism}}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheussdrefreshfailure",
              "summary": "Failed Prometheus SD refresh."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000090411,
            "lastEvaluation": "2025-05-28T20:39:25.961197894Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusKubernetesListWatchFailures",
            "query": "increase(prometheus_sd_kubernetes_failures_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Kubernetes service discovery of Prometheus {{$labels.namespace}}/{{$labels.pod}} is experiencing {{ printf \"%.0f\" $value }} failures with LIST/WATCH requests to the Kubernetes API in the last 5 minutes.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuskuberneteslistwatchfailures",
              "summary": "Requests in Kubernetes SD are failing."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000094562,
            "lastEvaluation": "2025-05-28T20:39:25.96129163Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusNotificationQueueRunningFull",
            "query": "(predict_linear(prometheus_notifications_queue_length{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]))",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull",
              "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000149341,
            "lastEvaluation": "2025-05-28T20:39:25.961389623Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
            "query": "(rate(prometheus_notifications_errors_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])) * 100 > 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ printf \"%.1f\" $value }}% of alerts sent by Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}} were affected by errors.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers",
              "summary": "More than 1% of alerts sent by Prometheus to a specific Alertmanager were affected by errors."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00016117,
            "lastEvaluation": "2025-05-28T20:39:25.96154225Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusNotConnectedToAlertmanagers",
            "query": "max_over_time(prometheus_notifications_alertmanagers_discovered{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) < 1",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers",
              "summary": "Prometheus is not connected to any Alertmanagers."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000072507,
            "lastEvaluation": "2025-05-28T20:39:25.961706339Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusTSDBReloadsFailing",
            "query": "increase(prometheus_tsdb_reloads_failures_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[3h]) > 0",
            "duration": 14400,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing",
              "summary": "Prometheus has issues reloading blocks from disk."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000216422,
            "lastEvaluation": "2025-05-28T20:39:25.961781938Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusTSDBCompactionsFailing",
            "query": "increase(prometheus_tsdb_compactions_failed_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[3h]) > 0",
            "duration": 14400,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing",
              "summary": "Prometheus has issues compacting blocks."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00014797,
            "lastEvaluation": "2025-05-28T20:39:25.96200333Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusNotIngestingSamples",
            "query": "(sum without (type) (rate(prometheus_tsdb_head_samples_appended_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])) <= 0 and (sum without (scrape_job) (prometheus_target_metadata_cache_entries{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}) > 0 or sum without (rule_group) (prometheus_rule_group_rules{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}) > 0))",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples",
              "summary": "Prometheus is not ingesting samples."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000430501,
            "lastEvaluation": "2025-05-28T20:39:25.962154661Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusDuplicateTimestamps",
            "query": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps",
              "summary": "Prometheus is dropping samples with duplicate timestamps."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000088215,
            "lastEvaluation": "2025-05-28T20:39:25.962589939Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOutOfOrderTimestamps",
            "query": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps",
              "summary": "Prometheus drops samples with out-of-order timestamps."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000100591,
            "lastEvaluation": "2025-05-28T20:39:25.96268137Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRemoteStorageFailures",
            "query": "((rate(prometheus_remote_storage_failed_samples_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])) / ((rate(prometheus_remote_storage_failed_samples_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])) + (rate(prometheus_remote_storage_succeeded_samples_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) or rate(prometheus_remote_storage_samples_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])))) * 100 > 1",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures",
              "summary": "Prometheus fails to send samples to remote storage."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000307579,
            "lastEvaluation": "2025-05-28T20:39:25.962784896Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRemoteWriteBehind",
            "query": "(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) - ignoring (remote_name, url) group_right () max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])) > 120",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind",
              "summary": "Prometheus remote write is behind."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000121599,
            "lastEvaluation": "2025-05-28T20:39:25.963096739Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRemoteWriteDesiredShards",
            "query": "(max_over_time(prometheus_remote_storage_shards_desired{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]))",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}` $labels.instance | query | first | value }}.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards",
              "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000087854,
            "lastEvaluation": "2025-05-28T20:39:25.96322117Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusRuleFailures",
            "query": "increase(prometheus_rule_evaluation_failures_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures",
              "summary": "Prometheus is failing rule evaluations."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000205282,
            "lastEvaluation": "2025-05-28T20:39:25.96331251Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusMissingRuleEvaluations",
            "query": "increase(prometheus_rule_group_iterations_missed_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations",
              "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00022832,
            "lastEvaluation": "2025-05-28T20:39:25.963520766Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusTargetLimitHit",
            "query": "increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because the number of targets exceeded the configured target_limit.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit",
              "summary": "Prometheus has dropped targets because some scrape configs have exceeded the targets limit."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000075753,
            "lastEvaluation": "2025-05-28T20:39:25.963753473Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusLabelLimitHit",
            "query": "increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit",
              "summary": "Prometheus has dropped targets because some scrape configs have exceeded the labels limit."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000082301,
            "lastEvaluation": "2025-05-28T20:39:25.96383212Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusScrapeBodySizeLimitHit",
            "query": "increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit",
              "summary": "Prometheus has dropped some targets that exceeded body size limit."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000067841,
            "lastEvaluation": "2025-05-28T20:39:25.963917486Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusScrapeSampleLimitHit",
            "query": "increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit",
              "summary": "Prometheus has failed scrapes that have exceeded the configured sample limit."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000064254,
            "lastEvaluation": "2025-05-28T20:39:25.963988283Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusTargetSyncFailure",
            "query": "increase(prometheus_target_sync_failed_total{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[30m]) > 0",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "{{ printf \"%.0f\" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure",
              "summary": "Prometheus has failed to sync targets."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000168962,
            "lastEvaluation": "2025-05-28T20:39:25.964055253Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusHighQueryLoad",
            "query": "avg_over_time(prometheus_engine_queries{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) > 0.8",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheushighqueryload",
              "summary": "Prometheus is reaching its maximum capacity serving concurrent requests."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000126668,
            "lastEvaluation": "2025-05-28T20:39:25.964226869Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusErrorSendingAlertsToAnyAlertmanager",
            "query": "min without (alertmanager) (rate(prometheus_notifications_errors_total{alertmanager!~\"\",job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m]) / rate(prometheus_notifications_sent_total{alertmanager!~\"\",job=\"kube-prometheus-stack-prometheus\",namespace=\"kube-prometheus\"}[5m])) * 100 > 3",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "{{ printf \"%.1f\" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager",
              "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000172115,
            "lastEvaluation": "2025-05-28T20:39:25.964356348Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.003665436,
        "lastEvaluation": "2025-05-28T20:39:25.960866306Z"
      },
      {
        "name": "prometheus-operator",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-kube-prometheus-stack-prometheus-operator-93f8b6c1-cc4a-4cc3-8dd0-aa273ae1cfac.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "PrometheusOperatorListErrors",
            "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[10m])) / sum by (cluster, controller, namespace) (rate(prometheus_operator_list_operations_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[10m]))) > 0.4",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors",
              "summary": "Errors while performing list operations in controller."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000628134,
            "lastEvaluation": "2025-05-28T20:39:21.491557241Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorWatchErrors",
            "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m])) / sum by (cluster, controller, namespace) (rate(prometheus_operator_watch_operations_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]))) > 0.4",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors",
              "summary": "Errors while performing watch operations in controller."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000306065,
            "lastEvaluation": "2025-05-28T20:39:21.492196381Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorSyncFailed",
            "query": "min_over_time(prometheus_operator_syncs{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\",status=\"failed\"}[5m]) > 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed",
              "summary": "Last controller reconciliation failed"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000119158,
            "lastEvaluation": "2025-05-28T20:39:21.492509592Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorReconcileErrors",
            "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_reconcile_errors_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]))) / (sum by (cluster, controller, namespace) (rate(prometheus_operator_reconcile_operations_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]))) > 0.1",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors",
              "summary": "Errors while reconciling objects."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000267817,
            "lastEvaluation": "2025-05-28T20:39:21.49263388Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorStatusUpdateErrors",
            "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_status_update_errors_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]))) / (sum by (cluster, controller, namespace) (rate(prometheus_operator_status_update_operations_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]))) > 0.1",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "{{ $value | humanizePercentage }} of status update operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorstatusupdateerrors",
              "summary": "Errors while updating objects status."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000250078,
            "lastEvaluation": "2025-05-28T20:39:21.492908689Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorNodeLookupErrors",
            "query": "rate(prometheus_operator_node_address_lookup_errors_total{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]) > 0.1",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors",
              "summary": "Errors while reconciling Prometheus."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000082065,
            "lastEvaluation": "2025-05-28T20:39:21.493163936Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorNotReady",
            "query": "min by (cluster, controller, namespace) (max_over_time(prometheus_operator_ready{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\"}[5m]) == 0)",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready",
              "summary": "Prometheus operator not ready"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000177597,
            "lastEvaluation": "2025-05-28T20:39:21.493248922Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PrometheusOperatorRejectedResources",
            "query": "min_over_time(prometheus_operator_managed_resources{job=\"kube-prometheus-stack-operator\",namespace=\"kube-prometheus\",state=\"rejected\"}[5m]) > 0",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf \"%0.0f\" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.",
              "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources",
              "summary": "Resources rejected by Prometheus operator"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000125062,
            "lastEvaluation": "2025-05-28T20:39:21.493431373Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.002042919,
        "lastEvaluation": "2025-05-28T20:39:21.491516576Z"
      },
      {
        "name": "mirth",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-mirth-bfcb14b4-aa56-43c0-aae7-2bcf01248292.yaml",
        "rules": [
          {
            "name": "mirth:message_errors_by_total",
            "query": "sum by (channelName) (rate(mirth_messages_errored_total[10m]) / rate(mirth_messages_received_total[10m]))",
            "health": "ok",
            "evaluationTime": 0.000255433,
            "lastEvaluation": "2025-05-28T20:39:31.17541597Z",
            "type": "recording"
          },
          {
            "state": "inactive",
            "name": "MirthMessageErrors",
            "query": "mirth:message_errors_by_total * 100 > 3",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "More than 3% of received messages errored",
              "summary": "Mirth Message Error"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000069289,
            "lastEvaluation": "2025-05-28T20:39:31.175677639Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "MirthMessagesQueued",
            "query": "mirth_messages_queued > 0",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Mirth is queuing messages for more than 10 minutes",
              "summary": "Mirth Messages Queued"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000039952,
            "lastEvaluation": "2025-05-28T20:39:31.175749785Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "MirthMemoryUsage",
            "query": "mirth_server_size > (17 * 1024 * 1024 * 1024)",
            "duration": 600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Mirth is using more than 17Gi in memory",
              "summary": "High Mirth Memory Usage"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000162098,
            "lastEvaluation": "2025-05-28T20:39:31.175792412Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000646186,
        "lastEvaluation": "2025-05-28T20:39:31.175310787Z"
      },
      {
        "name": "PostgresExporter",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-postgresql-rules-499af453-ddf2-40a6-8674-2e0d42573644.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "PostgresqlDown",
            "query": "pg_up == 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Postgresql instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql down (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000247205,
            "lastEvaluation": "2025-05-28T20:39:34.539739931Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlRestarted",
            "query": "time() - pg_postmaster_start_time_seconds < 60",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Postgresql restarted\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql restarted (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.0001049,
            "lastEvaluation": "2025-05-28T20:39:34.539993788Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlExporterError",
            "query": "pg_exporter_last_scrape_error > 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Postgresql exporter is showing errors. A query may be buggy in query.yaml\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql exporter error (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000055174,
            "lastEvaluation": "2025-05-28T20:39:34.54010409Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlTableNotAutoVacuumed",
            "query": "((pg_stat_user_tables_n_tup_del + pg_stat_user_tables_n_tup_upd + pg_stat_user_tables_n_tup_hot_upd) > pg_settings_autovacuum_vacuum_threshold) and (time() - pg_stat_user_tables_last_autovacuum) > 60 * 60 * 24 * 10",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Table {{ $labels.relname }} has not been auto vacuumed for 10 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql table not auto vacuumed (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000234939,
            "lastEvaluation": "2025-05-28T20:39:34.54016261Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlTableNotAutoAnalyzed",
            "query": "((pg_stat_user_tables_n_tup_del + pg_stat_user_tables_n_tup_upd + pg_stat_user_tables_n_tup_hot_upd) > pg_settings_autovacuum_analyze_threshold) and (time() - pg_stat_user_tables_last_autoanalyze) > 24 * 60 * 60 * 10",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Table {{ $labels.relname }} has not been auto analyzed for 10 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql table not auto analyzed (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000156588,
            "lastEvaluation": "2025-05-28T20:39:34.540401928Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlTooManyConnections",
            "query": "sum by (instance, job, server) (pg_stat_activity_count) > min by (instance, job, server) (pg_settings_max_connections * 0.8)",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "PostgreSQL instance has too many connections (> 80%).\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql too many connections (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000273955,
            "lastEvaluation": "2025-05-28T20:39:34.540562184Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlNotEnoughConnections",
            "query": "sum by (datname) (pg_stat_activity_count{datname!~\"template.*|postgres|test_new_lantern|mirth\"}) == 0",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "PostgreSQL instance should have more connections (> 0)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql not enough connections (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000221076,
            "lastEvaluation": "2025-05-28T20:39:34.540841022Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlDeadLocks",
            "query": "increase(pg_stat_database_deadlocks{datname!~\"template.*|postgres\"}[1m]) > 5",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "PostgreSQL has dead-locks\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql dead locks (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000134969,
            "lastEvaluation": "2025-05-28T20:39:34.54106583Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlCommitRateLow",
            "query": "increase(pg_stat_database_xact_commit{datid!=\"0\",datname!~\"template.*|postgres\"}[5m]) < 5",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Postgresql seems to be processing very few transactions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql commit rate low (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000153839,
            "lastEvaluation": "2025-05-28T20:39:34.541203978Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlLowXidConsumption",
            "query": "rate(pg_txid_current[1m]) < 5",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Postgresql seems to be consuming transaction IDs very slowly\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql low XID consumption (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000058038,
            "lastEvaluation": "2025-05-28T20:39:34.541363335Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlHighRateStatementTimeout",
            "query": "rate(postgresql_errors_total{type=\"statement_timeout\"}[1m]) > 3",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Postgres transactions showing high rate of statement timeouts\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql high rate statement timeout (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000056665,
            "lastEvaluation": "2025-05-28T20:39:34.541424272Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlHighRateDeadlock",
            "query": "increase(postgresql_errors_total{type=\"deadlock_detected\"}[1m]) > 1",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Postgres detected deadlocks\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql high rate deadlock (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00005462,
            "lastEvaluation": "2025-05-28T20:39:34.541484359Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlUnusedReplicationSlot",
            "query": "pg_replication_slots_active == 0",
            "duration": 60,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Unused Replication Slots\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql unused replication slot (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000044038,
            "lastEvaluation": "2025-05-28T20:39:34.541541507Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlTooManyDeadTuples",
            "query": "((pg_stat_user_tables_n_dead_tup > 10000) / (pg_stat_user_tables_n_live_tup + pg_stat_user_tables_n_dead_tup)) >= 0.1",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "PostgreSQL dead tuples is too large\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql too many dead tuples (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00010054,
            "lastEvaluation": "2025-05-28T20:39:34.541588123Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlConfigurationChanged",
            "query": "{__name__=~\"pg_settings_.*\"} != on (__name__, instance) {__name__=~\"pg_settings_([^t]|t[^r]|tr[^a]|tra[^n]|tran[^s]|trans[^a]|transa[^c]|transac[^t]|transact[^i]|transacti[^o]|transactio[^n]|transaction[^_]|transaction_[^r]|transaction_r[^e]|transaction_re[^a]|transaction_rea[^d]|transaction_read[^_]|transaction_read_[^o]|transaction_read_o[^n]|transaction_read_on[^l]|transaction_read_onl[^y]).*\"} offset 5m",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "description": "Postgres Database configuration change has occurred\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql configuration changed (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.010487003,
            "lastEvaluation": "2025-05-28T20:39:34.541691636Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlSslCompressionActive",
            "query": "sum(pg_stat_ssl_compression) > 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Database allows connections with SSL compression enabled. This may add significant jitter in replication delay. Replicas should turn off SSL compression via `sslcompression=0` in `recovery.conf`.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql SSL compression active (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000103588,
            "lastEvaluation": "2025-05-28T20:39:34.552193778Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlTooManyLocksAcquired",
            "query": "((sum(pg_locks_count)) / (pg_settings_max_locks_per_transaction * pg_settings_max_connections)) > 0.2",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Too many locks acquired on the database. If this alert happens frequently, we may need to increase the postgres setting max_locks_per_transaction.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql too many locks acquired (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000278653,
            "lastEvaluation": "2025-05-28T20:39:34.552303113Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlBloatIndexHigh(>80%)",
            "query": "pg_bloat_btree_bloat_pct > 80 and on (idxname) (pg_bloat_btree_real_size > 100000000)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The index {{ $labels.idxname }} is bloated. You should execute `REINDEX INDEX CONCURRENTLY {{ $labels.idxname }};`\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql bloat index high (> 80%) (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000089795,
            "lastEvaluation": "2025-05-28T20:39:34.552585054Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlBloatTableHigh(>80%)",
            "query": "pg_bloat_table_bloat_pct > 80 and on (relname) (pg_bloat_table_real_size > 200000000)",
            "duration": 3600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The table {{ $labels.relname }} is bloated. You should execute `VACUUM {{ $labels.relname }};`\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql bloat table high (> 80%) (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000063752,
            "lastEvaluation": "2025-05-28T20:39:34.552677722Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlInvalidIndex",
            "query": "pg_general_index_info_pg_relation_size{indexrelname=~\".*ccnew.*\"}",
            "duration": 21600,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The table {{ $labels.relname }} has an invalid index: {{ $labels.indexrelname }}. You should execute `DROP INDEX {{ $labels.indexrelname }};`\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql invalid index (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00007343,
            "lastEvaluation": "2025-05-28T20:39:34.552746072Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "PostgresqlReplicationLag",
            "query": "pg_replication_lag_seconds > 5",
            "duration": 30,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The PostgreSQL replication lag is high (> 5s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Postgresql replication lag (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000048713,
            "lastEvaluation": "2025-05-28T20:39:34.552822153Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.013165086,
        "lastEvaluation": "2025-05-28T20:39:34.539708677Z"
      },
      {
        "name": "celery",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-prometheus-rules-eff05bb6-2dc7-4394-b788-70799381454a.yaml",
        "rules": [
          {
            "name": "celery:task_failed:increase5m",
            "query": "sum by (name) (increase(celery_task_failed_total[5m]))",
            "health": "ok",
            "evaluationTime": 0.00064528,
            "lastEvaluation": "2025-05-28T20:39:11.129191289Z",
            "type": "recording"
          },
          {
            "name": "celery:task_succeeded:increase5m",
            "query": "sum by (name) (increase(celery_task_succeeded_total[5m]))",
            "health": "ok",
            "evaluationTime": 0.000364909,
            "lastEvaluation": "2025-05-28T20:39:11.129846264Z",
            "type": "recording"
          },
          {
            "name": "celery:task_failure_ratio5m",
            "query": "celery:task_failed:increase5m / (celery:task_failed:increase5m + celery:task_succeeded:increase5m)",
            "health": "ok",
            "evaluationTime": 0.000188738,
            "lastEvaluation": "2025-05-28T20:39:11.130215479Z",
            "type": "recording"
          },
          {
            "state": "inactive",
            "name": "CeleryTaskFailure1%",
            "query": "celery:task_failure_ratio5m > 0.01",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "info"
            },
            "annotations": {
              "summary": "More than 1% of the {{ $labels.name }} have failed"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000103603,
            "lastEvaluation": "2025-05-28T20:39:11.130408301Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CeleryTaskFailure5%",
            "query": "celery:task_failure_ratio5m > 0.05",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "summary": "More than 5% of the {{ $labels.name }} have failed"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000064714,
            "lastEvaluation": "2025-05-28T20:39:11.130515679Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CeleryTaskFailure10%",
            "query": "celery:task_failure_ratio5m > 0.1",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "error"
            },
            "annotations": {
              "summary": "More than 10% of the {{ $labels.name }} have failed"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000051879,
            "lastEvaluation": "2025-05-28T20:39:11.130582796Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CeleryHighQueueLength",
            "query": "sum by (job, namespace, queue_name) (celery_queue_length{job=~\".*celery.*\",queue_name!~\"None\"}) > 100",
            "duration": 1200,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "More than 100 tasks in the queue {{ $labels.job }}/{{ $labels.queue_name }} the past 20m.",
              "summary": "Celery high queue length."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000161787,
            "lastEvaluation": "2025-05-28T20:39:11.130637133Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "CeleryWorkerDown",
            "query": "celery_worker_up{job=~\".*celery.*\"} == 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The Celery worker {{ $labels.job }}/{{ $labels.hostname }} is offline.",
              "summary": "A Celery worker is offline."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000161429,
            "lastEvaluation": "2025-05-28T20:39:11.130803556Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.001823711,
        "lastEvaluation": "2025-05-28T20:39:11.129144327Z"
      },
      {
        "name": "django",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-prometheus-rules-eff05bb6-2dc7-4394-b788-70799381454a.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "DjangoMigrationsUnapplied",
            "query": "sum by (namespace, job) (django_migrations_unapplied_total{job=~\"django\"}) > 0",
            "duration": 900,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The job {{ $labels.job }} has unapplied migrations.",
              "summary": "Django has unapplied migrations."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000250173,
            "lastEvaluation": "2025-05-28T20:39:22.374574241Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "DjangoDatabaseException",
            "query": "sum by (type, namespace, job) (increase(django_db_errors_total{job=~\"django\"}[10m])) > 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The job {{ $labels.job }} has hit the database exception {{ $labels.type }}.",
              "summary": "Django database exception."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000156586,
            "lastEvaluation": "2025-05-28T20:39:22.374831885Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "DjangoHighHttp4xxErrorRate",
            "query": "sum by (namespace, job, view) (rate(django_http_responses_total_by_status_view_method_total{job=~\"django\",status=~\"^4.*\",view!~\"<unnamed view>|health_check:health_check_home|prometheus-django-metrics\"}[5m])) / sum by (namespace, job, view) (rate(django_http_responses_total_by_status_view_method_total{job=~\"django\",view!~\"<unnamed view>|health_check:health_check_home|prometheus-django-metrics\"}[5m])) * 100 > 2",
            "duration": 60,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "More than 2% HTTP requests with status 4xx for {{ $labels.job }}/{{ $labels.view }} the past 5m.",
              "summary": "Django high HTTP 4xx error rate."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00021043,
            "lastEvaluation": "2025-05-28T20:39:22.374993978Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "DjangoHighHttp5xxErrorRate",
            "query": "sum by (namespace, job, view) (rate(django_http_responses_total_by_status_view_method_total{job=~\"django\",status=~\"^5.*\",view!~\"<unnamed view>|health_check:health_check_home|prometheus-django-metrics\"}[5m])) / sum by (namespace, job, view) (rate(django_http_responses_total_by_status_view_method_total{job=~\"django\",view!~\"<unnamed view>|health_check:health_check_home|prometheus-django-metrics\"}[5m])) * 100 > 5",
            "duration": 60,
            "keepFiringFor": 0,
            "labels": {
              "severity": "error"
            },
            "annotations": {
              "description": "More than 5% HTTP requests with status 5xx for {{ $labels.job }}/{{ $labels.view }} the past 5m.",
              "summary": "Django high HTTP 5xx error rate."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00018875,
            "lastEvaluation": "2025-05-28T20:39:22.37520817Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "DjangoApiTooSlow",
            "query": "histogram_quantile(0.99, sum by (view, le) (rate(django_http_requests_latency_seconds_by_view_method_bucket{view!~\"(api:report-templates-match|api:study-generate-.*|core:spellcheck|silk:.*|prometheus-django-metrics)\"}[10m]))) > 5",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The 99th percentile of requests are running slower than 5s for the past 5 minutes",
              "summary": "The API performance has degraded"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002351919,
            "lastEvaluation": "2025-05-28T20:39:22.375400309Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "DjangoLLMEndpointTooSlow",
            "query": "histogram_quantile(0.99, sum by (view, le) (rate(django_http_requests_latency_seconds_by_view_method_bucket{view!~\"(silk:.*|prometheus-django-metrics)\"}[10m]))) > 15",
            "duration": 300,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "The 99th percentile of requests are running slower than 15s for the past 5 minutes",
              "summary": "The API performance for the LLM endpoints have degraded"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002090466,
            "lastEvaluation": "2025-05-28T20:39:22.377760041Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.00531189,
        "lastEvaluation": "2025-05-28T20:39:22.374542462Z"
      },
      {
        "name": "Oliver006RedisExporter",
        "file": "/etc/prometheus/rules/prometheus-kube-prometheus-stack-prometheus-rulefiles-0/kube-prometheus-redis-rules-4770871c-abed-4cee-9835-139b3f493c5b.yaml",
        "rules": [
          {
            "state": "inactive",
            "name": "RedisDown",
            "query": "redis_up == 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Redis instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Redis down (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000249737,
            "lastEvaluation": "2025-05-28T20:39:12.622332729Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "RedisOutOfSystemMemory",
            "query": "redis_memory_used_bytes / redis_total_system_memory_bytes * 100 > 90",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Redis is running out of system memory (> 90%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Redis out of system memory (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000132605,
            "lastEvaluation": "2025-05-28T20:39:12.622588065Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "RedisOutOfConfiguredMaxmemory",
            "query": "redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90 and on (instance) redis_memory_max_bytes > 0",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Redis is running out of configured maxmemory (> 90%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Redis out of configured maxmemory (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000142535,
            "lastEvaluation": "2025-05-28T20:39:12.622726413Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "RedisTooManyConnections",
            "query": "redis_connected_clients / redis_config_maxclients * 100 > 90",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Redis is running out of connections (> 90% used)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Redis too many connections (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000146536,
            "lastEvaluation": "2025-05-28T20:39:12.622871886Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "RedisNotEnoughConnections",
            "query": "redis_connected_clients < 5",
            "duration": 120,
            "keepFiringFor": 0,
            "labels": {
              "severity": "warning"
            },
            "annotations": {
              "description": "Redis instance should have more connections (> 5)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Redis not enough connections (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000047241,
            "lastEvaluation": "2025-05-28T20:39:12.623022346Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "RedisRejectedConnections",
            "query": "increase(redis_rejected_connections_total[1m]) > 0",
            "duration": 0,
            "keepFiringFor": 0,
            "labels": {
              "severity": "critical"
            },
            "annotations": {
              "description": "Some connections to Redis has been rejected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}",
              "summary": "Redis rejected connections (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000086949,
            "lastEvaluation": "2025-05-28T20:39:12.623072203Z",
            "type": "alerting"
          }
        ],
        "interval": 30,
        "limit": 0,
        "evaluationTime": 0.000863514,
        "lastEvaluation": "2025-05-28T20:39:12.622298565Z"
      }
    ]
  }
}
